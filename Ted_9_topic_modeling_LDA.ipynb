{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import unidecode\n",
    "%matplotlib inline\n",
    "import copy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora,models,similarities\n",
    "from gensim.utils import lemmatize\n",
    "from string import punctuation\n",
    "from spacy.parts_of_speech import ADV, NOUN, ADJ, PUNCT, VERB\n",
    "from spacy.en import English,STOPWORDS\n",
    "from spacy.orth import *\n",
    "import logging\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The api key is ynw2u8e4h9sk8c2htp7vutxq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook deals exclusively with LDA topic modeling, the previous steps such as tokenization, tf-idf are skipped, results are directly loaded from the other Ted_9 LSI notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "captions = pd.read_json('captions_f3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = captions['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iddict = dict([(item[1],int(item[0])) for item in ids.iteritems()])\n",
    "rowdict = dict([(int(item[0]),item[1]) for item in ids.iteritems()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load('./data/tedtrain.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpustrain = corpora.MmCorpus('./data/corpustrain.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1531"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpustrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Latent Dirichlet Allocation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters of LDA models were determined based on both human evaluation and log perpelexity of the hold-out set. First, a hyperparameter grid of {number of topics, alpha, eta} was set up and similar talks were drawn for two talks (ids 1666 and 129), for which I had obtained results using LSI and the results were validated. The similar talks given by LDA were compared with that given by LSI, and the LDA models that produced best matched similar talks were selected. Second, a holdout set (the test set used in the rating prediction task) is used to test the perpelexity and the best model was chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def give_similar_talks(talkid, num_of_talks,corpus,index):\n",
    "    \"\"\"\n",
    "    return a list of tuples, (talkid, cosine similarity score in descending order)\n",
    "    \"\"\"\n",
    "    sims = index[corpus[iddict[talkid]]]\n",
    "    sims = sorted(enumerate(sims),reverse = True, key = lambda x:x[1])\n",
    "    sims_id = [(rowdict[key],value) for key,value in sims]\n",
    "    return sims_id[:num_of_talks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topic_doc(projection,ids,num_top_topics,talkid = True):\n",
    "    \"\"\"\n",
    "    print the most relevant topics for the chosen documents\n",
    "    either row id or talkid can be used as input\n",
    "    \"\"\"\n",
    "    if talkid:\n",
    "        rowids = [iddict[x] for x in ids]\n",
    "    else:\n",
    "        rowids = ids\n",
    "    for ctr in xrange(len(rowids)):\n",
    "        proj = projection[rowids[ctr]]\n",
    "        proj.sort(reverse = True, key = lambda x:abs(x[1]))\n",
    "        print 'The top {} topics of the {}th document'.format(num_top_topics,rowids[ctr])\n",
    "        if talkid:\n",
    "            print 'The talk id is {}'.format(ids[ctr])\n",
    "        print proj[:num_top_topics]\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the topic distribution by talks 1666 and 129, the former is an education talk the latter is a high-tech talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_list = [1,3,10,'symmetric','auto','asymmetric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta_list = [0.3,1,3,10,'auto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topic_list = [18,20,25,30,40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nested_list = [num_topic_list,alpha_list,eta_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_list = list(product(*nested_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid_list has 150 hyperparameters combination in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grid_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up two dictionary to store (hyparameter-combination, [similar talks]) pair for the two targeted talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_talks_1666 = defaultdict(list)\n",
    "sim_talks_129 = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finishing the 0th combination in 91.8138279915 sec\n",
      "finishing the 1th combination in 101.844889879 sec\n",
      "finishing the 2th combination in 101.399289131 sec\n",
      "finishing the 3th combination in 110.304780006 sec\n",
      "finishing the 4th combination in 98.7791631222 sec\n",
      "finishing the 5th combination in 78.1259589195 sec\n",
      "finishing the 6th combination in 84.3467669487 sec\n",
      "finishing the 7th combination in 92.0698041916 sec\n",
      "finishing the 8th combination in 80.6833298206 sec\n",
      "finishing the 9th combination in 86.8609139919 sec\n",
      "finishing the 10th combination in 70.6086568832 sec\n",
      "finishing the 11th combination in 74.3404428959 sec\n",
      "finishing the 12th combination in 75.9294681549 sec\n",
      "finishing the 13th combination in 72.9085729122 sec\n",
      "finishing the 14th combination in 86.2294661999 sec\n",
      "finishing the 15th combination in 114.409653902 sec\n",
      "finishing the 16th combination in 115.273396969 sec\n",
      "finishing the 17th combination in 123.436055183 sec\n",
      "finishing the 18th combination in 115.410548925 sec\n",
      "finishing the 19th combination in 116.778422832 sec\n",
      "finishing the 20th combination in 110.240437984 sec\n",
      "finishing the 21th combination in 117.012893915 sec\n",
      "finishing the 22th combination in 114.760010004 sec\n",
      "finishing the 23th combination in 116.731364012 sec\n",
      "finishing the 24th combination in 115.986179829 sec\n",
      "finishing the 25th combination in 106.93659687 sec\n",
      "finishing the 26th combination in 116.345036983 sec\n",
      "finishing the 27th combination in 114.945394993 sec\n",
      "finishing the 28th combination in 109.455476999 sec\n",
      "finishing the 29th combination in 115.040443897 sec\n",
      "finishing the 30th combination in 89.6088690758 sec\n",
      "finishing the 31th combination in 100.781591892 sec\n",
      "finishing the 32th combination in 107.32801795 sec\n",
      "finishing the 33th combination in 104.16862011 sec\n",
      "finishing the 34th combination in 99.4860270023 sec\n",
      "finishing the 35th combination in 84.2830159664 sec\n",
      "finishing the 36th combination in 94.0785419941 sec\n",
      "finishing the 37th combination in 90.0998589993 sec\n",
      "finishing the 38th combination in 84.2143728733 sec\n",
      "finishing the 39th combination in 87.5021400452 sec\n",
      "finishing the 40th combination in 71.1965689659 sec\n",
      "finishing the 41th combination in 76.2786900997 sec\n",
      "finishing the 42th combination in 76.2523741722 sec\n",
      "finishing the 43th combination in 76.4973158836 sec\n",
      "finishing the 44th combination in 85.9423708916 sec\n",
      "finishing the 45th combination in 107.18650198 sec\n",
      "finishing the 46th combination in 118.838475943 sec\n",
      "finishing the 47th combination in 115.994504929 sec\n",
      "finishing the 48th combination in 115.240720034 sec\n",
      "finishing the 49th combination in 115.020331144 sec\n",
      "finishing the 50th combination in 107.422493935 sec\n",
      "finishing the 51th combination in 121.256363869 sec\n",
      "finishing the 52th combination in 118.220062017 sec\n",
      "finishing the 53th combination in 117.277529001 sec\n",
      "finishing the 54th combination in 116.597211838 sec\n",
      "finishing the 55th combination in 109.011671066 sec\n",
      "finishing the 56th combination in 119.357012987 sec\n",
      "finishing the 57th combination in 117.072426081 sec\n",
      "finishing the 58th combination in 110.831134081 sec\n",
      "finishing the 59th combination in 114.970612049 sec\n",
      "finishing the 60th combination in 92.2038481236 sec\n",
      "finishing the 61th combination in 104.358903885 sec\n",
      "finishing the 62th combination in 104.373565197 sec\n",
      "finishing the 63th combination in 107.946723938 sec\n",
      "finishing the 64th combination in 100.010284185 sec\n",
      "finishing the 65th combination in 85.0555372238 sec\n",
      "finishing the 66th combination in 86.2533929348 sec\n",
      "finishing the 67th combination in 89.0639128685 sec\n",
      "finishing the 68th combination in 77.1800482273 sec\n",
      "finishing the 69th combination in 88.7966439724 sec\n",
      "finishing the 70th combination in 72.6870670319 sec\n",
      "finishing the 71th combination in 76.1717309952 sec\n",
      "finishing the 72th combination in 75.0179491043 sec\n",
      "finishing the 73th combination in 74.6856989861 sec\n",
      "finishing the 74th combination in 83.5384049416 sec\n",
      "finishing the 75th combination in 107.1910882 sec\n",
      "finishing the 76th combination in 120.115586996 sec\n",
      "finishing the 77th combination in 115.497673988 sec\n",
      "finishing the 78th combination in 106.386718035 sec\n",
      "finishing the 79th combination in 116.463953018 sec\n",
      "finishing the 80th combination in 109.589922905 sec\n",
      "finishing the 81th combination in 122.548058033 sec\n",
      "finishing the 82th combination in 120.749688148 sec\n",
      "finishing the 83th combination in 96.5169429779 sec\n",
      "finishing the 84th combination in 122.319652796 sec\n",
      "finishing the 85th combination in 116.70318985 sec\n",
      "finishing the 86th combination in 120.426867962 sec\n",
      "finishing the 87th combination in 113.481745958 sec\n",
      "finishing the 88th combination in 108.713029861 sec\n",
      "finishing the 89th combination in 118.162926912 sec\n",
      "finishing the 90th combination in 93.8499310017 sec\n",
      "finishing the 91th combination in 104.84315896 sec\n",
      "finishing the 92th combination in 110.174623966 sec\n",
      "finishing the 93th combination in 88.4563891888 sec\n",
      "finishing the 94th combination in 103.589496136 sec\n",
      "finishing the 95th combination in 85.9932618141 sec\n",
      "finishing the 96th combination in 86.0401639938 sec\n",
      "finishing the 97th combination in 88.2277109623 sec\n",
      "finishing the 98th combination in 76.4905011654 sec\n",
      "finishing the 99th combination in 90.7466759682 sec\n",
      "finishing the 100th combination in 73.3924100399 sec\n",
      "finishing the 101th combination in 74.9280340672 sec\n",
      "finishing the 102th combination in 74.4168670177 sec\n",
      "finishing the 103th combination in 72.5114619732 sec\n",
      "finishing the 104th combination in 82.3975419998 sec\n",
      "finishing the 105th combination in 110.919019938 sec\n",
      "finishing the 106th combination in 124.380369902 sec\n",
      "finishing the 107th combination in 121.010767221 sec\n",
      "finishing the 108th combination in 112.267637014 sec\n",
      "finishing the 109th combination in 123.681831837 sec\n",
      "finishing the 110th combination in 115.773694038 sec\n",
      "finishing the 111th combination in 127.107925177 sec\n",
      "finishing the 112th combination in 123.774886847 sec\n",
      "finishing the 113th combination in 120.304210901 sec\n",
      "finishing the 114th combination in 121.79065609 sec\n",
      "finishing the 115th combination in 112.903038025 sec\n",
      "finishing the 116th combination in 134.27926302 sec\n",
      "finishing the 117th combination in 117.913107157 sec\n",
      "finishing the 118th combination in 111.391695023 sec\n",
      "finishing the 119th combination in 131.039707899 sec\n",
      "finishing the 120th combination in 104.63459301 sec\n",
      "finishing the 121th combination in 116.535220146 sec\n",
      "finishing the 122th combination in 121.335472107 sec\n",
      "finishing the 123th combination in 112.062105894 sec\n",
      "finishing the 124th combination in 109.849946976 sec\n",
      "finishing the 125th combination in 90.2376728058 sec\n",
      "finishing the 126th combination in 96.4229278564 sec\n",
      "finishing the 127th combination in 96.4498500824 sec\n",
      "finishing the 128th combination in 80.9123618603 sec\n",
      "finishing the 129th combination in 102.313603163 sec\n",
      "finishing the 130th combination in 81.1721050739 sec\n",
      "finishing the 131th combination in 80.865167141 sec\n",
      "finishing the 132th combination in 76.7673280239 sec\n",
      "finishing the 133th combination in 74.7302660942 sec\n",
      "finishing the 134th combination in 86.7796342373 sec\n",
      "finishing the 135th combination in 121.366543055 sec\n",
      "finishing the 136th combination in 127.102852106 sec\n",
      "finishing the 137th combination in 129.064356089 sec\n",
      "finishing the 138th combination in 125.205092192 sec\n",
      "finishing the 139th combination in 127.747884035 sec\n",
      "finishing the 140th combination in 117.596238136 sec\n",
      "finishing the 141th combination in 127.91641593 sec\n",
      "finishing the 142th combination in 127.708890915 sec\n",
      "finishing the 143th combination in 109.345284939 sec\n",
      "finishing the 144th combination in 130.179254055 sec\n",
      "finishing the 145th combination in 118.94137001 sec\n",
      "finishing the 146th combination in 128.163959026 sec\n",
      "finishing the 147th combination in 124.46509409 sec\n",
      "finishing the 148th combination in 117.25783205 sec\n",
      "finishing the 149th combination in 131.462005138 sec\n"
     ]
    }
   ],
   "source": [
    "for ind, comb in enumerate(grid_list):\n",
    "    time0 = time()\n",
    "    lda_trial = models.LdaModel(corpustrain,num_topics=comb[0],id2word = dictionary, alpha = comb[1],\n",
    "                               eta = comb[2],chunksize = 200, iterations = 150,passes = 20)\n",
    "    \n",
    "    corpustrain_lda_tmp = lda_trial[corpustrain]\n",
    "    index_tmp = similarities.MatrixSimilarity(corpustrain_lda_tmp)\n",
    "    sim_talks_1666[comb] = give_similar_talks(1666,5,corpustrain_lda_tmp,index_tmp)\n",
    "    sim_talks_129[comb] = give_similar_talks(129,5,corpustrain_lda_tmp,index_tmp)\n",
    "    print 'finishing the {}th combination in {} sec'.format(ind,time()-time0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1666, 1.0),\n",
       " (1403, 0.96541113),\n",
       " (1136, 0.93835831),\n",
       " (1672, 0.9308055),\n",
       " (1954, 0.93054229)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_talks_1666[(30,'asymmetric','auto')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(129, 1.0),\n",
       " (766, 0.95296592),\n",
       " (481, 0.94456315),\n",
       " (1841, 0.93380558),\n",
       " (1244, 0.93007463)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_talks_129[(18,1,0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(10, 1, 0.3)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sim_talks_129.keys()).difference(set(sim_talks_1666.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the two defaultdicts: sim_talks_1666 and sim_talks_129 as pickle files\n",
    "sim_talks_129 has one extra key (10,1,0.3) which is not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(sim_talks_1666,open('./data/sim_talks_1666.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(sim_talks_129,open('./data/sim_talks_129.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim_talks_1666 = pickle.load(open('./data/sim_talks_1666.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim_talks_129 = pickle.load(open('./data/sim_talks_129.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For both talks, select the combination of hyperparameters that generate best matched similar talks compared with the results given by LSI\n",
    "The LSI results were generated in Ted_9_topic_modeling_LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1666, the top-5 returned similar talks from LSI (num_topics = 20, 50) are [1672,1954,1403,930,1032]\n",
    "\n",
    "For 129, the top-6 returned similar talks from LSI (num_topics = 20,50 ) are [766,139,210,481,1902,826]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_set_1666 = {1672,1954,1403,930,1032,1666}\n",
    "sim_set_129 = {766,139,210,481,1902,826,129}\n",
    "# the first-pass solution does not take into account the relative ranking of the similar talks in these two sets,\n",
    "# only concerns the set difference, related to Jaccard distance\n",
    "res_1666 = []\n",
    "for key,value in sim_talks_1666.iteritems():\n",
    "    sim_talks_lda = set([x[0] for x in value])\n",
    "    shared_talks = sim_talks_lda.intersection(sim_set_1666)\n",
    "    res_1666.append((key,len(shared_talks)))\n",
    "res_1666.sort(reverse = True,key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_129 = []\n",
    "for key,value in sim_talks_129.iteritems():\n",
    "    sim_talks_lda = set([x[0] for x in value])\n",
    "    shared_talks = sim_talks_lda.intersection(sim_set_129)\n",
    "    res_129.append((key,len(shared_talks)))\n",
    "res_129.sort(reverse = True,key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select LDA models that give consistent good similarity predictions for the two talks compared to the results generated by LSI. For talk 1666, the prediction should have at least 4 talks (including the talk itself) in common with the LSI prediction; for the talk 129, the prediction should have at least 3 talks (including the talk itself) in common with the LSI prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_res_1666 = []\n",
    "sel_res_129 = []\n",
    "for i in xrange(150):\n",
    "    if res_1666[i][1] >= 4:\n",
    "        sel_res_1666.append(res_1666[i])\n",
    "    if res_129[i][1] >= 3:\n",
    "        sel_res_129.append(res_129[i])\n",
    "set_1666 = set(dict(sel_res_1666).keys())\n",
    "set_129 = set(dict(sel_res_129).keys())\n",
    "comm_set = set_1666.intersection(set_129)\n",
    "# combine the key in comm_set with the num of matched similar talks\n",
    "comm_res = [(key,(dict(sel_res_1666)[key],dict(sel_res_129)[key])) for key in comm_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(25, 3, 'auto'),\n",
       " (30, 1, 0.3),\n",
       " (30, 'asymmetric', 'auto'),\n",
       " (40, 3, 0.3),\n",
       " (40, 'asymmetric', 'auto')}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rerun those selected models with higher number of iterations and record its log-perplexity change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finishing up the 0th key calculation in 95.7564558983 seconds\n",
      "the last evaluated log-perplexity is -16.898\n",
      "finishing up the 1th key calculation in 119.44173193 seconds\n",
      "the last evaluated log-perplexity is -10.270\n",
      "finishing up the 2th key calculation in 128.505738974 seconds\n",
      "the last evaluated log-perplexity is -10.100\n",
      "finishing up the 3th key calculation in 83.674382925 seconds\n",
      "the last evaluated log-perplexity is -20.446\n",
      "finishing up the 4th key calculation in 86.5635120869 seconds\n",
      "the last evaluated log-perplexity is -10.374\n"
     ]
    }
   ],
   "source": [
    "corpuslist = []\n",
    "ctr = 0\n",
    "for key in comm_set:\n",
    "    time0 = time()\n",
    "    lda_tmp = models.LdaModel(corpustrain,num_topics=key[0],id2word = dictionary, alpha = key[1],\n",
    "                               eta = key[2],chunksize = 200, iterations = 200,passes = 20)\n",
    "    corpus_lda_tmp = lda_tmp[corpustrain]\n",
    "    corpuslist.append((key,[lda_tmp,corpus_lda_tmp]))\n",
    "    print 'finishing up the {}th key calculation in {} seconds'.format(ctr,time()-time0)\n",
    "    print 'the last evaluated log-perplexity is {:.3f}'.format(lda_tmp.log_perplexity(corpustrain[1400:]))\n",
    "    ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpuslist = corpuslist[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finishing up the 5th key calculation in 90.1669118404 seconds\n",
      "the last evaluated log-perplexity is -10.323\n"
     ]
    }
   ],
   "source": [
    "time0 = time()\n",
    "lda_tmp = models.LdaModel(corpustrain,num_topics=25,id2word = dictionary, alpha = 3,\n",
    "                           eta = 'auto',chunksize = 200, iterations = 600,passes = 20)\n",
    "corpus_lda_tmp = lda_tmp[corpustrain]\n",
    "corpuslist.append(((25,3,'auto'),[lda_tmp,corpus_lda_tmp]))\n",
    "print 'finishing up the {}th key calculation in {} seconds'.format(ctr,time()-time0)\n",
    "print 'the last evaluated log-perplexity is {:.3f}'.format(lda_tmp.log_perplexity(corpustrain[1400:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "    The better models are (25, 3,'auto'),(30,'asymmetric','auto') and (40,'asymmetric','auto') according to the internal log-perplexity calculation conducted during model training using an internal holdout set\n",
    "    Due to the smaller size, I finally picked the model with 25 topics for visualization purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the models, the transformed corpus and the data frames\n",
    "This is for the purpose of visualization and future follow-up tasks such as prediction, therefore the model is only trained with the training set. The results were saved in the r2py folder, was originally intended for the dirichlet regression rate prediciton using R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in corpuslist:\n",
    "    key = item[0]\n",
    "    keystr = '_'.join([str(x) for x in key])\n",
    "    lda_tmp,corpus_tmp = item[1][0],item[1][1]\n",
    "    topic_df = pd.DataFrame([dict(x) for x in corpus_tmp])\n",
    "    corpuspath = './r2py/corpustrain_lda_'+ keystr + '.mm'\n",
    "    modelpath = './r2py/lda_'+keystr+'.model'\n",
    "    dfpath = './r2py/ldafeatures_'+keystr+'.csv'\n",
    "    corpora.MmCorpus.serialize(corpuspath,corpus_tmp)\n",
    "    lda_tmp.save(modelpath)\n",
    "    topic_df.to_csv(dfpath,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final look at the similar talks given by each model for talks 1666 and 129 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_tmp = models.LdaModel.load('./r2py/lda_25_3_auto.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tmp = corpora.MmCorpus('./r2py/corpustrain_lda_25_3_auto.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_tmp = similarities.MatrixSimilarity(corpus_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim1666 = give_similar_talks(1666,10,corpus_tmp,index_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim129 = give_similar_talks(129,10,corpus_tmp,index_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1666, 1.0),\n",
       " (1403, 0.99174792),\n",
       " (1672, 0.9789983),\n",
       " (1954, 0.95567638),\n",
       " (1033, 0.95540977),\n",
       " (297, 0.95210242),\n",
       " (1248, 0.95129913),\n",
       " (1040, 0.95107841),\n",
       " (1596, 0.95061839),\n",
       " (2074, 0.93506551)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim1666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(129, 1.0),\n",
       " (139, 0.93286169),\n",
       " (766, 0.9226687),\n",
       " (1366, 0.89578968),\n",
       " (481, 0.87153292),\n",
       " (1266, 0.86575675),\n",
       " (1515, 0.8590467),\n",
       " (1630, 0.85715783),\n",
       " (937, 0.85688257),\n",
       " (1958, 0.8541826)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter identical topics  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are possibly some extremely weak topics (e.g., with all-zero word-topic distribution) in some of these models, for the purpose of being used as feature columns in future prediction model (e.g., prediction of ratings), those topic columns (likely to be colinear) need to be filtered out and keep only one such topic. Use correlation matrix to detect such columns and save the filtered document-topic dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filtertopic(topic_df):\n",
    "    topic_df.fillna(0,inplace = True)\n",
    "    corrmatrix = np.round(np.array(topic_df.corr()),decimals = 4)\n",
    "    row,col = np.where(corrmatrix == 1)\n",
    "    duplicated = set([tp[1] for tp in zip(row,col) if tp[0] < tp[1]])\n",
    "    filtered_tp_df = topic_df.drop(duplicated,axis = 1)\n",
    "    return filtered_tp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in corpuslist:\n",
    "    key = item[0]\n",
    "    keystr = '_'.join([str(x) for x in key])\n",
    "    corpus_tmp = item[1][1]\n",
    "    tp_df = pd.DataFrame([dict(x) for x in corpus_tmp])\n",
    "    ftp_df = filtertopic(tp_df)\n",
    "    dfpath = './r2py/fldafeatures_'+keystr+'.csv'\n",
    "    ftp_df.to_csv(dfpath,index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
